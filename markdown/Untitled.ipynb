{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<head>\n",
    "  <h2>Background Image</h2>\n",
    "    <style=\"background-image:url('gazeview.jpg'); background-size: cover; min-height: 500px; background-attachment: fixed; background-position: right top; background-repeat:no-repeat;\"> \n",
    "    </style>\n",
    "-->\n",
    "<body>\n",
    "<div class=\"background\" style=\"background-image:url('gazeview.jpg'); background-size: cover; min-height: 500px; background-attachment: fixed; background-position: right top; background-repeat:no-repeat;\"> \n",
    "\n",
    "<!--\n",
    "<head>\n",
    "<link href=\"format.css\" type=\"text/css\" rel=\"stylesheet\" />    \n",
    "</head>    \n",
    "-->   \n",
    "\n",
    "<!--\n",
    "<style>\n",
    "body{\n",
    "background-image:url('gazeview.jpg'); \n",
    "  background-size: cover; \n",
    "  min-height: 500px; \n",
    "  background-attachment: fixed; \n",
    "  background-position: right top; \n",
    "  background-repeat:no-repeat;    \n",
    "}\n",
    "-->     \n",
    "\n",
    "\n",
    "## Σκοπός Ειδικού Θέματος\n",
    "\n",
    "\n",
    "\n",
    "* Πειραματική εξέταση του Αλγορίθμου __Random Forests__ στο πρόβλημα του Gaze Recognition\n",
    "\n",
    "\n",
    "* Αναφορά των δυνατών και αδύνατων σημείων του αλγορίθμου\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Αναφορά στο πρόβλημα του  Gaze Recognition\n",
    "\n",
    "\n",
    "* <b>Gaze Recognition</b> ή <b>Gaze tracking</b> είναι μια διαδικασία όπου προσπαθούμε να μετρήσουμε την <b>κατεύθυνση</b> του βλέμματος ενός ματιού ή το <b>σημείο</b> που αυτό κοιτάζει\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Πρακτικές εφαρμογές του Gaze Recognition \n",
    "\n",
    "\n",
    "\n",
    "#####  1) Χρήση της τεχνολογίας από άτομα με κινητικά προβλήματα. Το μάτι λειτουργεί ως ποντίκι/διακόπτης\n",
    "\n",
    "\n",
    "\n",
    "* Για κάποιους ανθρώπους με κινητικά προβλήματα, τα μάτια ίσως είναι το μοναδικό μέσο επικοινωνίας με το περιβάλλον\n",
    "\n",
    "\n",
    "* Το μάτι θα μπορούσε να παίξει τον ρόλο ενός διακόπτη μέχρι και πιο πολύπλοκων λειτουργιών, όπως τον ρόλο που έχει το ποντίκι σε έναν υπολογιστή [7]   \n",
    "\n",
    "\n",
    "#####  2) Χρήση της τεχνολογίας για να ελέγξουμε κατά πόσο κάποιος είναι συγκεντρωμένος σε κάτι\n",
    "\n",
    "\n",
    "\n",
    "* Για παράδειγμα, μπορούμε να καταλάβουμε κατά πόσο οι θεατές μιας παράστασης βρίσκουν ενδιαφέρουσα την παράσταση για παράδειγμα [8]\n",
    "\n",
    "\n",
    "* Μια ιστιοσελίδα που φιλοξενεί διαφημίσεις, θα μπορούσε να μετρά το αντίκτυπο της διαφήμισης με βάση το χρόνο που την κοιτάει το μάτι αντί για τον αριθμό των επισκέψεων της σελίδας   \n",
    "\n",
    "\n",
    "\n",
    "#####  3) Χρήση της τεχνολογίας για να προβλέψουμε την δράση κάποιου ατόμου, με δεδομένο το Gaze\n",
    "\n",
    "\n",
    "* Αν μελετήσουμε μια αλληλουχία κινήσεων των ματιών, μπορούμε να προβλέψουμε τις δράσεις ενός ατόμου\n",
    "\n",
    "\n",
    "* Σύμφωνα με το [9], αυτό μπορεί να εφαρμοστεί σε καθημέρινες εργασίες(διάβασμα, σερφάρισμα στο internet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####  4) Διάφορες άλλες εφαρμογές\n",
    "\n",
    "\n",
    "* Gaze Tracking κατά τη διάρκεια προσομοιώσεων\n",
    "\n",
    "\n",
    "* Χρήση της τεχνολογίας του gaze tracking από συστήματα αυτοκινήτων με στόχο την πρόβλεψη δράσεων του οδηγού\n",
    "\n",
    "\n",
    "* Χρήση της τεχνολογίας του gaze tracking από τη βιομηχανία ηλεκτρονικών παιχνιδιών\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Δεδομένα\n",
    "\n",
    "* Ως δεδομένα επέλεξα το <a href=\"https://www.mpi-inf.mpg.de/de/abteilungen/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/\"\n",
    "target=\"_blank\">MPIIGaze Dataset</a><a href=\"https://www.mpi-inf.mpg.de/de/abteilungen/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/\"\n",
    "target=\"_blank\">MPIIGaze Dataset</a> [^3]. Ωστόσο υπάρχουν κι'άλλα dataset, όπως το <a href=\"https://www.idiap.ch/dataset/eyediap\" target=\"_blank\">Eyediap</a> και το <a href=\"http://www.hci.iis.u-tokyo.ac.jp/datasets/\" target=\"_blank\">Multiview Dataset</a> [^2].\n",
    "\n",
    "\n",
    "* Οι αρχικές εικόνες έχουνε κανονικοποιηθεί με τέτοιο τρόπο, ώστε να εξετάζονται όλες οι εικόνες υπό τις __ίδιες συνθήκες__ .Επίσης κάθε μάτι εξετάζεται __ανεξάρτητα__ από το άλλο.\n",
    "\n",
    "\n",
    "* Τα δεδομένα που έχουμε στην διάθεση μας είναι:\n",
    "\n",
    "    1. Οι εικόνες __e__ του κάθε ματιού με διαστάσεις (W,H) = (60,36)\n",
    "\n",
    "\t2. __Ηead Pose__, 2d διάνυσμα γωνιών σε radians(γωνία Theta και γωνία\n",
    "Phi)\n",
    "\n",
    "\t3. __Gaze__(2d διάνυσμα επίσης σε radians) το όποιο προσπαθούμε να κάνουμε predict. Κάθε μάτι γίνεται predict __ανεξάρτητα__ από το άλλο\n",
    "\n",
    "\t4. Η γωνία __Theta__ εκφράζει την οριζόντια θέση του κεφαλιού. Για\n",
    "παράδειγμα αν το κεφάλι έχει προσανατολισμό  προς τα __δεξιά__, θα έχει\n",
    "__θετική__ τιμή, ενώ αν κοιτάει προς τα __αριστερά__, θα έχει __αρνητική__.\n",
    "\n",
    "\t5. Η γωνία __Phi__ λειτουργεί σαν την Theta, αλλά για τον κάθετο άξονα.\n",
    "Για παράδειγμα, αν το κεφάλι έχει προσανατολισμό προς τα __πάνω__, θα έχει\n",
    "__θετική τιμή__, ενώ αν κοιτάει προς τα __κάτω__, θα έχει __αρνητική__\n",
    "\n",
    "\t6. Και οι 2 αυτές γωνίες κυμαίνονται στο διάστημα [-30, +30] σε\n",
    "__μοίρες__\n",
    "\n",
    "\n",
    "* Για τον αλγόριθμο Random Forest, κάνουμε __reshape__ τις εικόνες των ματιών\n",
    "  από (W,H) = (__60,36__) σε (__15,9__) τόσο για το __training__, όσο και για το __testing__  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Υλοποίηση Αλγορίθμου\n",
    "\n",
    "\n",
    "* Για την υλοποίηση του  αλγορίθμου, βασίστηκα στην αρχική υλοποίηση του Breiman[^1], κάνοντας κάποιες αλλαγές στον τρόπο που διαλέγουμε τα __features__ κατά το split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ομαδοποίηση των δεδομένων με βάση τα Head Poses\n",
    "\n",
    "\n",
    "* Για την υλοποίηση του  αλγορίθμου, αρχικά ομαδοποιούμε τα training samples σε __P pose clusters__, με βάση το __Head Pose__\n",
    "\n",
    "\n",
    "* Κάθε Cluster έχει ένα __κέντρο__, το οποίο αποτελείται από ένα διάνυσμα\n",
    "  (__theta, phi__)\n",
    "\n",
    "\n",
    "* Για να θεωρηθεί ένα διανύσμα (__theta, phi__) ως κέντρο ενός Cluster, θα πρέπει να <b>μην απέχει</b> απόσταση μικρότερη από <b>Χ</b> από τα ήδη υπάρχοντα κέντρα(πχ στο παρακάτω σχήμα χρησιμοποιώ __Χ=0.08__ και δημιουργούνται __106 Clusters__).\n",
    "\n",
    "\n",
    "* Όσο __μικρότερο__ το Χ, τόσο πιο __πολλά__ Clusters δημιουργούνται</br>\n",
    "\n",
    "<html><br>\n",
    "<div id=\"foto\" style=\"text-align: center;\">\n",
    "   <img src=\"centers.jpg\"  width=\"900\" alt=\"foto1\">\n",
    "        <figcaption><b>Εικόνα 1</b>:<i> Διάγραμμα που απεικονίζει τα  <b>Head Poses</b> όλων των σημείων του Training Phase. Με <b>πράσινο</b> χρώμα απεικονίζονται τα κέντρα των Clusters, ενώ με <b>μπλέ</b> χρώμα τα υπόλοιπα σημεία. Η παραπάνω εικόνα χρησιμοποιεί <b>44640</b> training δείγματα, ενώ τα <b>κέντρα</b> απέχουν μεταξύ τους απόσταση <b>μεγαλύτερη</b> των <b>0.03 radians</b>(1.718873) μοίρες)</i></figcaption>\n",
    "</div>\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "## Κατασκευή του δάσους μέσα από Regression Decision Trees\n",
    "\n",
    "* Χρησιμοποιώ την __bootstrap__ διαδικασία, επιλέγοντας τυχαία inputs\n",
    "\n",
    "\n",
    "* Δημιουργούμε τόσα __δέντρα__, όσα και τα __Pose Clusters__, δηλαδή P\n",
    "\n",
    "\n",
    "* Κάθε δέντρο παίρνει training data από τα __R-nearest Clusters__. Δηλαδή R Clusters\n",
    "  με τα __κοντινότερα__ Head Poses\n",
    "\n",
    "\n",
    "* Ως __error__ παίρνουμε το __μέσο gaze error__ από όλα τα regression trees.\n",
    "\n",
    "\n",
    "<div id=\"foto\" style=\"text-align: center;\">\n",
    "   <img src=\"visualization.jpeg\" width=\"500\" alt=\"foto1\">\n",
    "   <figcaption><b>Εικόνα 2</b>:<i> Παράδειγμα όπου <b>γειτονικά Clusters</b> συνεισφέρουν στην κατασκευή ενός δέντρου. Στα Clusters ανοίκουν δείγματα με <b>παρόμοια Head Poses</b></i></br></figcaption>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Πώς εκπαιδεύεται το κάθε δέντρο του δάσους\n",
    "\n",
    "\n",
    "* Σε κάθε κόμβο ενός δέντρου, προσπαθούμε να μάθουμε __συναρτήσεις__ της μορφής\n",
    "\n",
    "$$\n",
    "    f = px1 - px2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Τα px1, px2 είναι οι __Gray__ τιμές από 2 pixel της eye Image (W=15,H=9).\n",
    "\n",
    "\n",
    "* Τα __pixels__ αυτά μαθαίνονται μέσα από το training. \n",
    "\n",
    "\n",
    "* Επίσης προσπαθούμε να \"μάθουμε\" το __βέλτιστο threshold τ__ για κάθε κόμβο, όπου:\n",
    "\n",
    "    1. αν $ f < τ $, τότε το training sample κατευθύνεται στο __αριστερό__ υποδέντρο\n",
    "    \n",
    "\t2. αν $ f >= τ $, τότε κατευθύνεται στο __δεξιό__ υποδέxντρο\n",
    "\n",
    "\n",
    "* Ο αλγόριθμος με τον οποίο υπολογίζουμε ποια είναι τα __βέλτιστα pixels__ και το __βέλτιστο threshold__ για το split σε __κάθε κόμβο__ του δέντρου είναι το __minimum residual sum of squares__\n",
    "\n",
    "\n",
    "$$ \\begin{align}\n",
    " error =\\sum_{\\substack{i:f_{j}\\lt{thres}}}^{nleft}  (g_{i} - \\hat{ m_{left} } )^2 + \\sum_{\\substack{i:f_{j}\\ge{thres}}}^{nright} (g_{i} - \\hat{ m_{right} } )^2\\end{align} $$\n",
    "\n",
    "* Τα $nleft$ και $nright$ είναι ο __αριθμός__ των δειγμάτων που θα είχε κάθε υποδέντρο, σε περίπτωση που γινόταν το split με βάση τα $px1$,$px2$,$thres$\n",
    "\n",
    "\n",
    "* Τα  $\\hat{ m_{right} }$ και $\\hat{ m_{left} }$ είναι η __μέση τιμή__ των gazes που ανήκουν στο __δεξί__ και __αριστερό__ υποδέντρο\n",
    "\n",
    "\n",
    "* Διαλέγουμε τα $px1$,$px2$,$thres$ που __ελαχιστοποιούν__ το παραπάνω άθροισμα\n",
    "\n",
    "\n",
    "<html><br>\n",
    "<div id=\"foto\" style=\"text-align: center;\">\n",
    "   <img src=\"stigmiotupo.png\" width=\"800\" alt=\"foto1\">\n",
    "    <figcaption><b>Εικόνα 3:</b><i>Στιγμιότυπο υποδέντρου <b>10 δειγμάτων</b>. Ανάλογα με τις τιμές των <b>Pixel</b> του δείγματος, το τελευταίο θα οδηγηθεί σε έναν <b>τερματικό κόμβο</b>(φύλλο)</i></br></br> </figcaption>\n",
    "</div>\n",
    "</html>\n",
    "\n",
    "\n",
    "* Γενικότερα στα <b>random forests</b>(Breiman 1984, regression and classification trees), σε προβλήματα regression, προτιμάται ο αλγόριθμος των <b>resindual sum of squares</b>. Αντίθετα με τα <b>regression</b> forests, στα <b>classification</b> προβλήματα η κατασκευή των δέντρων βασίζεται σε άλλες μεθόδους όπως το <b>tree entropy/gain</b> ή το <b>Gini impurity index</b>\n",
    "\n",
    "\n",
    "\n",
    "* Στόχος των <b>sum of squares</b> είναι να δημιουργηθεί το split, ώστε να πετυχαίνεται στα υποδέντρα το <b>μικρότερο δυνατό variance</b>\n",
    "\n",
    "\n",
    "* Ο τρόπος μάθησης των στοιχείων διαχωρισμού περιγράφεται παρακάτω:\n",
    " \n",
    "<html><br>\n",
    "<div id=\"foto\" style=\"text-align: center;\">\n",
    "   <img src=\"pseudocode.png\" alt=\"foto1\">\n",
    "    <figcaption><b>Εικόνα 4:</b><i>Στιγμιότυπο υποδέντρου <b>10 δειγμάτων</b>. Ανάλογα με τις τιμές των <b>Pixel</b> του δείγματος, το τελευταίο θα οδηγηθεί σε έναν <b>τερματικό κόμβο</b>(φύλλο)</i></br></br> </figcaption>\n",
    "</div>\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "      \n",
    "                1. Για κάθε δυνατό ζευγάρι pixel(px1,px2)\n",
    "                    2. Για κάθε threshold\n",
    "                        3. Υπολόγισε το rightError= sum of squares error στο δεξί υποδέντρο  \n",
    "                        4. Υπολόγισε το leftError για το αριστερό υποδέντρο\n",
    "                        5. Error = rightError + leftError\n",
    "                        6. Αν Error < minError\n",
    "                            7. minError = Error\n",
    "                            8. minPx1 = px1\n",
    "                            9. minPx2 = px2\n",
    "                            10. minThreshold = threshold\n",
    "-->                                  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Οπότε έτσι μαθαίνουμε τα  minPx1, minPx2, minThreshold κάθε κόμβου\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Πώς γίνεται το testing\n",
    "\n",
    "\n",
    "\n",
    "* Μόλις θέλουμε να ελέγξουμε ένα testing sample, δεν το στέλνουμε σε όλα τα\n",
    "  δέντρα, αλλά στα __R-nearest δέντρα__ με βάσει το head pose\n",
    "\n",
    "\n",
    "* Τότε υπολογίζουμε το __average error__ από τα R-nearest regression δέντρα.\n",
    "\n",
    "\n",
    "* Μας ενδιαφέρει ωστόσο να γνωρίζουμε και την __τυπική απόκλιση__(standard deviation), για να βλέπουμε __πόσο κοντά__ είναι οι προβλέψεις μας στο __mean error__\n",
    "\n",
    "\n",
    "\n",
    "## Πειραματική Αξιολόγηση Αλγορίθμου\n",
    "\n",
    "\n",
    "\n",
    "* Κατά την αναλυτική αξιολόγηση του Αλγορίθμου θα πρέπει να απαντήσουμε τα εξής ερωτήματας:\n",
    "\n",
    "    1. Ποιός είναι ο βέλτιστος __αριθμός__ από __Clusters__ ή ισοδύναμα ποιά θα είναι η __μικρότερη δυνατή απόσταση__ μεταξύ 2 κέντρων\n",
    "\n",
    "    2. Ποιός είναι ο βέλτιστος __αριθμός γειτόνων__ κάθε Cluster\n",
    "\n",
    "    3.  Ποιός είναι ο βέλτιστος __αριθμός δειγμάτων__ εκπαίδευσης\n",
    "\n",
    "    4. Κατά τη διαδικασία δημιουργίας υποδέντρων σε ένα δέντρο, πόσες __μεταβλητές διαχωρισμού__ χρησιμοποιούμε?\n",
    "\n",
    "    5. Πόσο __βάθος__ πρέπει να έχει το κάθε δέντρου\n",
    "\n",
    "    6. Αν σε ένα δέντρο καταλήξουμε σε φύλλο όπου υπάρχουν  __παραπάνω από ένα__ δείγματα, πώς προσδιορίζεται η 2d-gaze του συγκεκριμένου δέντρου\n",
    "\n",
    "    7. Υπάρχει τρόπος να προσδιορίσουμε όλα τα παραπάνω με κάποια μέθοδο/αλγόριθμο;\n",
    "\n",
    "##### Ερώτημα 1ο: Εύρεση μικρότερης απόστασης Κέντρων/Αριθμός Clusters\n",
    "\n",
    "\n",
    "* Επειδή δεν έχουμε υπολογίσει ακόμα τον __βέλτιστο αριθμό γειτόνων__, θεωρώ αρχικά πως __R = 5__ γείτονες.\n",
    "\n",
    "\n",
    "* Επίσης, για να μειώσω τον χρόνο εκπαίδευσης, χρησιμοποιώ αρχικά __10,000 training samples__\n",
    "\n",
    "\n",
    "* Ο αριθμός των __μεταβλητών διαχωρισμού__ που θα χρησιμοποιούσαμε είναι <b>WIDTH</b> * <b>HEIGHT</b> * <b>THRESHOLD_RANGE</b> = 15 * 16 * 255. Επειδή ο αριθμός αυτός είναι υπερβολικά μεγάλος, σύμφωνα με το paper του <b>Breiman</b> αρκεί χρησιμοποιήσουμε την __τετραγωνική ρίζα__ αυτού του αριθμού\n",
    "\n",
    "\n",
    "* Τέλος, αν ένα φύλλο ενός δέντρου περιέχει __παραπάνω από ένα__ training samples, υπολογίζω απλώς τον __μέσο όρο__ των __2-d gazes__ και προκύπτει ένα 2d-gaze διάνυσμα\n",
    "\n",
    "\n",
    "* Δεν περιορίζουμε το βάθος του κάθε δέντρου\n",
    "\n",
    "\n",
    "* Με βάση τα παραπάνω, υπολογίζουμε το __mean error__ και το __standard deviation__\n",
    "\n",
    "<html><br>\n",
    "<div id=\"foto\" style=\"text-align: center;\">\n",
    "    <img src=\"graph1.jpg\" height=\"400\" width=\"600\" alt=\"foto1\">\n",
    "    <figcaption><b>Εικόνα 5:</b><i>Διάγραμμα με οριζόντιο άξονα την <b>ελάχιστη απόσταση</b> σε rad ανάμεσα σε 2 κέντρα των γειτονικών Cluster συναρτήσει του <b>μέσου σφάλματος</b> και της <b>τυπικής απόκλισης</b>. Για απόσταση <b>d=0.05</b> rad παρατηρούμε το ελάχιστο μέσο σφάλμα. H τυπική απόκλιση είναι <b>υψηλή</b>(=5.31 μοίρες)</i></br></br> </figcaption>\n",
    "</div>\n",
    "</html>\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "* Από το γράφημα παρατηρούμε πως το <b>mean error</b> γίνεται ελάχιστο για όταν η minimum απόσταση ανάμεσα στα κέντρα είναι <b>0.05 degrees</b>(οριζόντια + κάθετα)\n",
    "\n",
    "\n",
    "* Ωστόσο η <b>τυπική απόκλιση</b> είναι αρκετά μεγάλη(5.38 μοίρες), πράγμα που σημαίνει πως οι προβλέψεις έχουν αρκετή <b>διαφορά</b> μεταξύ τους ως προς τον μέσο όρο\n",
    "\n",
    "\n",
    "* Κρατάμε ως απόσταση το 0.05 μεταξύ των κεντρών. Θέτοντας την απόσταση των κεντρών(των Cluster) 0.05, για 10,000 δείγματα εκπαίδευσης προκύπτουν <b>238 κέντρα</b> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Ερώτημα 2ο: Βέλτιστος Αριθμός γειτονικών Cluster/ αριθμός δέντρων\n",
    "\n",
    "\n",
    "\n",
    "* Ο συνολικός αριθμός των <b>Cluster</b> είναι ίσος με τον συνολικό αριθμό <b>δέντρων</b>\n",
    "\n",
    "\n",
    "* Κρατάμε ως δεδομένα την <b>minimum απόσταση</b> μεταξύ 2 κέντρων(=<b>0.05</b> rads) που βρήκαμε στο προηγούμενο ερώτημα και συνεχίζουμε τις μετρήσεις, αναζητώντας τον ιδανικό <b>αριθμό γειτονικών Cluster</b> που θα συνεισφέρουν στην εκπαίδευση ενός δέντρου.\n",
    "\n",
    "\n",
    "\n",
    "* Όπως και στο προηγούμενο ερώτημα, ο αριθμός των δειγμάτων εκπαίδευσης παραμένει <b>10,000</b>\n",
    "\n",
    "\n",
    "<html><br>\n",
    "<div id=\"foto\" style=\"text-align: center;\">\n",
    "    <img src=\"graph2.jpg\" height=\"400\" width=\"600\" alt=\"foto1\">\n",
    "    <figcaption><b>Εικόνα 6:</b><i>Διάγραμμα με οριζόντιο άξονα τον <b>αριθμό γειτόνων</b> που συνεισφέρουν στην εκπαίδευση ενός δέντρου συναρτήσει του <b>μέσου σφάλματος</b> και της <b>τυπικής απόκλισης</b>. Το ελάχιστο μέσο σφάλμα παρατηρείται για <b>R = 30</b>. Από το σημείο αυτό και μετά η μείωση θεωρείται αμελητέα. H τυπική απόκλιση εξακολουθεί να παραμένει αρκετά <b>υψηλή</b>(περίπου 5 μοίρες)</i></br></br> </figcaption>\n",
    "</div>\n",
    "\n",
    "\n",
    "* Παρατηρούμε πως η <b>αύξηση</b> του αριθμού των <b>γειτόνων</b>(μέχρι ένα σημείο) προκαλεί <b>μείωση</b> του <b>σφάλματος</b>\n",
    "\n",
    "\n",
    "* Αυτό συμβαίνει διότι καθώς <b>αυξάνουμε</b> τον αριθμό γειτόνων, <b>περισσότερα</b> δέντρα συνεισφέρουν στο τελικό αποτέλεσμα που θέλουμε να προβλέψουμε\n",
    "\n",
    "\n",
    "* Έτσι, παίρνοντας ως αποτέλεσμα την <b>μέση</b> πρόβλεψη όλων των δέντρων, <b>μικραίνει</b> η πιθανότητα να έχουμε <b>λανθασμένες</b> προβλέψεις\n",
    "\n",
    "\n",
    "* Αυτό είναι το σημείο που <b>υπερτερούν</b> τα random forests σε σχέση με τα decision trees. Τα <b>decision trees</b> από την φύση τους μπορούν <b>εύκολα</b> να κάνουν μία <b>λάθος</b> πρόβλεψη. Στα <b>random forests</b>, αν ένα δέντρο κάνει μια \"κακή πρόβλεψη\", τα υπόλοιπα δέντρα είναι σε θέση να <b>διορθώσουν</b> την πρόβλεψη αυτή, αφού παίρνουμε τον <b>μέσο όρο</b> όλων των δέντρων\n",
    "\n",
    "\n",
    "* Γενικά, όσο πιο <b>πολλά</b> είναι τα δέντρα τόσο πιο <b>καλή</b> θα είναι η πρόβλεψη. Απλά από ένα σημείο και μετά <b>δεν</b> θα έχει νόημα να <b>αυξάνουμε</b> τα δέντρα, γιατί η αύξυση θα γίνεται <b>ελάχιστη</b> σε σχέση με τον επιπρόσθετο <b>χρόνο</b> εκπαίδευσης που χρειαζόμαστε(<b>συγκλίνει</b> δηλαδή σε κάποιο αριθμό η επίδοση)\n",
    "\n",
    "##### Ερώτημα 3ο: Ποιός είναι ο βέλτιστος αριθμός δειγμάτων εκπαίδευσης\n",
    "\n",
    "\n",
    "* Θεωρώντας δεδομένο τον <b>αριθμό γειτόνων</b>(R=5) και την <b>ελάχιστη απόσταση</b> των κέντρων των Clusters(dist=0.05), προσπαθούμε να βρούμε τον βέλτιστο <b>αριθμό δειγμάτων</b> εκπαίδευσης.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<html><br>\n",
    "<div id=\"foto\" style=\"text-align: center;\">\n",
    "    <img src=\"graph3.jpg\" height=\"400\" width=\"600\" alt=\"foto1\">\n",
    "    <figcaption><b>Εικόνα 6:</b><i>Διάγραμμα με οριζόντιο άξονα τον <b>αριθμό δειγμάτων εκπαίδευσης</b> συναρτήσει του <b>μέσου σφάλματος</b> και της <b>τυπικής απόκλισης</b>. Το μέσο σφάλμα <b>συγκλίνει</b> στην τιμή <b>6.49</b> μοίρες, ενώ η τυπική απόκλιση παραμένει γύρω στις <b>4.7</b> μοίρες</i></br></br> </figcaption>\n",
    "</div>\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " * Σύμφωνα με τον <b>Breiman</b>, τα random forests <b>αποφεύγουν</b> το <b>overfitting</b> καθώς αυξάνεται το <b>training sample</b>\n",
    "\n",
    "\n",
    "\n",
    "* Οπότε παίρνουμε ως ολικό <b>μέσο</b> σφάλμα <b>6.49247</b> μοίρες με <b>τυπική απόκλιση</b> ως προς αυτό <b>4.70</b> μοίρες\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Ερώτημα 4ο: Εύρεση αριθμού μεταβλητών διαχωρισμού(split features)\n",
    "\n",
    "\n",
    "\n",
    "* <b>Μεταβλητές διαχωρισμού</b>(split features) είναι μεταβλητές οι οποίες καθορίζουν <b>πώς</b> θα γίνει το split(διαχωρισμός training samples) σε κάθε κόμβο του δέντρου. \n",
    "\n",
    "\n",
    "* Στο συγκεκριμένο πρόβλημα, χρησιμοποιούμε τις εξής <b>μεταβλητές διαχωρισμού</b>:\n",
    "\n",
    "    1. Τις <b>συντεταγμένες</b>(height,width) του <b>pixel 1</b>\n",
    "    \n",
    "\t2. Τις <b>συντεταγμένες</b>(height,width) του <b>pixel 2</b>\n",
    "    \n",
    "    3. Την <b>απόσταση</b>(threshold) ανάμεσα στις <b>gray</b> τιμές(0 -> 255 εύρος) αυτών των 2 pixel\n",
    "\n",
    "\n",
    "* Υπάρχουν <b>height*width</b> πιθανές τιμές που μπορούν να πάρουν  η 1η και η 2η μεταβλητή \n",
    "\n",
    "\n",
    "* Η μεταβλητή <b>threshold</b> έχει εύρος τιμών(thres_range) από <b>0</b> μέχρι <b>50</b>(αυθαίρετο το 50)\n",
    "\n",
    "\n",
    "* Κατά την διαδικασία της εκμάθησης, είπαμε πως προσπαθούμε να μάθουμε τί <b>τιμή</b> θα έχουν αυτές οι μεταβλητές σε κάθε <b>μή-τερματικό</b> κόμβο του κάθε δέντρου \n",
    "\n",
    "\n",
    "* Ωστόσο, υπολογιστικά θα ήταν δύσκολο να δοκιμάσω  $thresholdRange * (height*width)^2 = 50*(9*15)^2 = 911,250$ <b>διαφορετικές</b> παραμέτρους, ώστε να τις χρησιμοποιήσω στον τύπο για τον υπολογισμό του minimum square error\n",
    "\n",
    "\n",
    "* Αυτό λύνεται σύμφωνα με τον <b>Breiman</b>(2001), o oποίος αναφέρει πως δεν χρειάζεται να δοκιμάσουμε <b>όλους</b> τους συνδιασμούς των παραμέτρων. Αρκεί να χρησιμοποιήσουμε την <b>τετραγωνική ρίζα</b> αυτών, επιλέγοντας <b>τυχαίους</b> συνδιασμούς(στην <b>εικόνα 4</b>, o αλγόριθμος ανταποκρίνεται για τους <b>911,250</b> συνδιασμούς. Στην πράξη όμως χρησιμοποιούμε την <b>τετραγωνική ρίζα</b> αυτών) \n",
    "\n",
    "\n",
    "\n",
    "* Πράγματι, παρόλο που <b>δεν</b> πραγματοποιούμε όλους τους συνδιασμούς, η απόδοση του αλγορίθμου παραμένει η <b>υψηλότερη</b> όταν χρησιμοποιούμε την τετραγωνική ρίζα των συνδιασμών\n",
    "\n",
    "\n",
    "\n",
    "##### Ερώτημα 5ο: Βέλτιστο βάθος κάθε δέντρου\n",
    "\n",
    "\n",
    "* Ο πιο διαδεδομένος τρόπος για περιορισμό του <b>βάθους</b> ενός δέντρου είναι να θεωρήσουμε πως σε ένα φύλλο πρέπει να υπάρχουν <b>τουλάχιστον N</b> δείγματα, με N >= 1. \n",
    "\n",
    "\n",
    "* Έτσι μπορούμε να περιορίσουμε το <b>μέγεθος</b> ενός δέντρου, καθώς και να μειώσουμε τον <b>χρόνο εκπαίδευσης</b> του\n",
    "\n",
    "\n",
    "*******************\n",
    "\n",
    "διάγραμμα εδώ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Σύμφωνα με τον <b>Breiman</b>(2001), το βάθος κάθε δέντρου δεν θα πρέπει να περιορίζεται\n",
    "\n",
    "\n",
    "* Αντίθετα με τα <b>δέντρα αποφάσεως</b> όπου χωρίς περιορισμό του βάθους υπάρχει κίνδυνος <b>overfitting</b> [^5], στα random forests <b>δεν</b> είναι τόσο σημαντίκος\n",
    "\n",
    "\n",
    "* Ο λόγος είναι το ότι αν περιορίσουμε το <b>μέγεθος</b> των δέντρων, τότε τα δέντρα θα αρχίσουν να <b>μοιάζουν</b> μεταξύ τους, οπότε δεν έχει νόημα το έχουμε πολλά <b>όμοια</b> δέντρα\n",
    "\n",
    "\n",
    "* Πολλά <b>όμοια</b> δέντρα σημαίνει ότι όλα τα δέντρα παίρνουν την <b>ίδια</b> απόφαση, οπότε ο αλγόριθμος μετατρέπεται στον αλγόριθμο των <b>decision trees</b> αλλά με πολλά <b>ίδια</b> δέντρα\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Ερώτημα 6ο: Πώς προσδιορίζουμε την 2-gaze ενός δέντρου, αν σε ένα φύλλο υπάρχουν παραπάνω από ένα training samples\n",
    "\n",
    "\n",
    "\n",
    "* Στους παραπάνω υπολογισμούς, χρησιμοποίησα τον μέσο όρο των 2d-gazes από όλα τα training samples που βρισκόντουσαν σαυτό το φύλλο του δέντρου\n",
    "\n",
    "\n",
    "* Εκτός από τον μέσο όρο των δειγμάτων, αρκετά διαδεδομένη μέθοδος είναι το logistic regression \n",
    "\n",
    "\n",
    "\n",
    "## Σύγκριση σε σχέση με τις μετρήσεις των Sugano, Zhang\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Πιθανές βελτιώσεις του αλγορίθμου\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####   1) Μείωση του αριθμού των features\n",
    "\n",
    "\n",
    "To avoid over-fitting in random forest, the main thing you need to do is optimize a tuning parameter that governs the number of features that are randomly chosen to grow each tree from the bootstrapped data. Typically, you do this via k-fold cross-validation, where k∈{5,10}, and choose the tuning parameter that minimizes test sample prediction error. In addition, growing a larger forest will improve predictive accuracy, although there are usually diminishing returns once you get up to several hundreds of trees.\n",
    "\n",
    "* Και στις 2 περιπτώσεις, παρατηρείται το overfitting(παραπάνω δεδομένα εκπαίδευσης\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####   2) Μείωση του αριθμού των features\n",
    "\n",
    "\n",
    "* Όπως αναφέραμε και στο <b>ερώτημα 4</b>, ο αριθμός των features που χρησιμοποιούμε είναι αρκετά <b>μεγάλος</b>(=911,250). \n",
    "\n",
    "\n",
    "* Αν και ο Breiman(2001) θεωρεί πως η εισαγωγή της <b>τυχαιότητας</b> κατά την επιλογή των features σε συνδιασμό με τον <b>μεγάλο αριθμό δέντρων</b>(γειτόνων στο δικό μας πρόβλημα) μπορεί να <b>εξαλείψει</b> το πρόβλημα του μεγάλου αριθμού από features, θα μπορούσαμε ωστόσο πειραματικά να εφαρμόσουμε αλγορίθμους που <b>μειώνουν</b> τον αριθμό των <b>features</b>(dimensionality reduction) πριν τρέξουμε τον αλγόριθμο μας(πχ. PCA, LDA) και να ελέγξουμε κατά πόσο οι αλγόριθμοι αυτοί μειώνουν το <b>ολικό error</b>. \n",
    "\n",
    "\n",
    "* Με αυτόν τον τρόπο, θα μπορούσαμε κατά την κατασκευή ενός <b>μη-τερματικού</b> κόμβου να κάνουμε <b>απαλοιφή</b> κάποιων features ώστε να επικεντρωθούμε σε <b>σημαντικά</b> features\n",
    "\n",
    "\n",
    "* Υπάρχει ωστόσο ο κίνδυνος να δημιουργούνται κατά το training <b>όμοια</b> δέντρα, πράγμα που αφαιρεί το νόημα του αλγορίθμου(καταλήγουμε στα decision trees, όπου η διακύμανση είναι μεγάλη)\n",
    "\n",
    "\n",
    "\n",
    "#####    3) Κάθε πρόβλεψη ενός δέντρου έχει έναν συντελεστή \"αυτοπεποίθησης\"\n",
    "\n",
    "\n",
    "* Η τεχνική αυτή παρουσιάζεται στο [^4], όπου κάθε <b>πρόβλεψη</b> ενός μεμονωμένου δέντρου, συνοδεύεται από έναν συντελεστή όμοιο με την <b>τυπική απόκλιση</b>\n",
    "\n",
    "\n",
    "* Αν στο <b>τερματικό φύλλο</b> ενός δέντρου ανήκουν <b>παραπάνω από 1</b> training samples, υπολογίζουμε την <b>τυπική απόκλιση</b> των training samples αυτών ως προς την <b>μέση τιμή</b>\n",
    "\n",
    "\n",
    "* Αν η τιμή της τυπικής απόκλισης <b>ξεπερνά</b> μια <b>προκαθορισμένη</b> τιμή, τότε η πρόβλεψη θεωρείται <b>αναξιόπιστη</b> και το δάσος <b>απορρίπτει</b> την πρόβλεψη αυτού του δέντρου. \n",
    "\n",
    "\n",
    "* Ο <b>τελικός</b> μέσος όρος στην περίπτωση αυτή υπολογίζεται <b>χωρίς</b> να λαμβάνουμε υπ'όψη το δέντρο αυτό\n",
    "\n",
    "\n",
    "* Με αυτόν τον τρόπο, <b>απορρίπτουμε</b> προβλέψεις οι οποίες είναι <b>αμφίβολες</b>, καθώς δεν αντικατοπτρίζεται ο μέσος όρος από τα training samples\n",
    "\n",
    "\n",
    "#####   4) Χρήση κάποιας συνάρτησης αντί του μέσου όρου στα φύλλα των δέντρων\n",
    "\n",
    "\n",
    "* Στις περιπτώσεις όπου το φύλλο ενός δέντρου περιέχει παραπάνω από ένα training samples, θα μπορούσαμε να χρησιμοποιήσουμε για τον προσδιορισμό του τελικού 2d-gaze διανύσματος κάποια συνάρτηση, αντί για τον υπολογισμού του μέσου gaze\n",
    "\n",
    "\n",
    "* Ανάλογα με την <b>κατανομή</b> των δεδομένων, θα μπορούσαμε να επιλέξουμε την κατάλληλη συνάρτηση, όπως για παράδειγμα logistic regression, SVMs, ή non-linear συναρτήσεις\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 5) Συντελεστές βαρών \n",
    "\n",
    "SVMs, grid search, grid search, random search,\n",
    "bayesian optimization, frequentist vs bayesian\n",
    "acquisition function, posterior distribution of f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Λογισμικό που σχετίζεται με τα random forests\n",
    "\n",
    "\n",
    "\n",
    "* Για να πραγματοποιηθούν οι παραπάνω μετρήσεις, υλοποίησα τον αλγόριθμο των Random Forests αρχικά σε <b>Matlab</b>. Ωστόσο επειδή η απόδοση <b>δεν</b> ήταν η αναμενόμενη, προχώρησα στην υλοποίηση του αλγορίθμου σε <b>C/C++</b> ώστε να στοχεύσω στην <b>χρονική απόδοση</b> \n",
    "\n",
    "\n",
    "* Ο λόγος που προχώρησα στην <b>υλοποίηση</b> του αλγορίθμου αντί να χρησιμοποιήσω κάποια από τα <b>παρακάτω</b> πακέτα είναι πως μέσα από την υλοποίηση θα υπήρχε καλύτερη <b>κατανόηση</b> του αλγορίθμου. Επιπλέον επειδή είναι ένας σχετικά <b>εύκολα υλοποιήσιμος</b> αλγόριθμος σε σχέση με άλλους αλγορίθμους, η υλοποίηση έγινε σε <b>λογικά χρονικά πλαίσια</b>\n",
    "\n",
    "\n",
    "* Το λογισμικό που υλοποιεί τον αρχικό σχεδιασμό των random forests όπως ο <b>Breiman</b>(2001) είχε ορίσει, βρίσκεται στην ιστιοσελίδα του <a href=\"https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\"\n",
    "target=\"_blank\">Berkeley</a> και είναι γραμμένο σε <b>Fortran</b>\n",
    "\n",
    "\n",
    "* Αρκετά δημοφιλές είναι το πακέτο <a href=\"https://cran.r-project.org/web/packages/randomForest/index.html\"\n",
    "target=\"_blank\">Random Forests</a> σε γλώσσα <b>R</b> \n",
    "\n",
    "\n",
    "* Υπάρχει επίσης για <b>Python</b>, από το πακέτο <b>Scikit-learn</b> συναρτήσεις για τα <a href=\"http://scikit-learn.org/stable/modules/ensemble.html#random-forests\" target=\"_blank\">Random Forests</a> \n",
    "\n",
    "\n",
    "*  Σε <b>Matlab</b>, υπάρχει η συνάρτηση <a href=\"http://in.mathworks.com/help/stats/treebagger.html?refresh=true\"\n",
    "target=\"_blank\">TreeBagger</a>\n",
    "\n",
    "\n",
    "* Τέλος, υπάρχει η κλάση <a href=\"https://docs.opencv.org/2.4/modules/ml/doc/random_trees.html\"\n",
    "target=\"_blank\">RandomTrees</a> της <b>OpenCV</b> για υλοποίηση σε <b>C++</b>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Aναφορές σε βιβλιογραφίες/δημοσιεύσεις\n",
    "\n",
    "\n",
    "#####  Δημοσιεύσεις πάνω στα Random Forests  και στο Gaze Tracking\n",
    "\n",
    "[^1]: Breiman, L., Friedman, J.,Olshen, R., and Stone, C. [1984] Classification and Regression Trees,  Wadsworth\n",
    "[^2]: Y. Sugano, Y. Matsushita, and Y. Sato. [2014] [Learning-by-synthesis for appearance-based 3d gaze estimation.\n",
    "[^3]: Z. Zhang, Y.Sugano, M.Fritz, A. Bulling [2015] Appearance-Based Gaze Estimation in the Wild\n",
    "[^4]: M. Dantone, J. Gall, G. Fanelli, L. Van Gool [2013]:  Real-time  facial feature detection using conditional regression forests.\n",
    "[^5]: Mansour, Y. [1997], Pessimistic decision tree pruning based on tree size\n",
    "[^6]: Philipp Probst, Marvin Wright and Anne-Laure Boulesteix. [2018], Hyperparameters and Tuning Strategies for Random Forest\n",
    "\n",
    "\n",
    "#####  Δημοσιεύσεις σχετικά με εφαρμογές του Gaze Tracking\n",
    "\n",
    "[^7]: P. Majaranta, A. Bulling [2014] Eye Tracking and Eye-Based Human–Computer Interaction\n",
    "[^8]: Z. Zhang, Y.Sugano, A. Bulling [2016] AggreGaze: Collective Estimation of Audience Attention on Public Displays \n",
    "[^9]:  H. Sattar, S. Müller, M. Fritz, A. Bulling [2015] Prediction of Search Targets From Fixations in Open-World Settings\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "</body>\n",
    "<!--\n",
    "</style>  \n",
    "-->    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
