
<!--
<head>
  <h2>Background Image</h2>
    <style="background-image:url('gazeview.jpg'); background-size: cover; min-height: 500px; background-attachment: fixed; background-position: right top; background-repeat:no-repeat;"> 
    </style>
-->
<body>
<div class="background" style="background-image:url('gazeview.jpg'); background-size: cover; min-height: 500px; background-attachment: fixed; background-position: right top; background-repeat:no-repeat;"> 

<!--
<head>
<link href="format.css" type="text/css" rel="stylesheet" />    
</head>    
-->   

<!--
<style>
body{
background-image:url('gazeview.jpg'); 
  background-size: cover; 
  min-height: 500px; 
  background-attachment: fixed; 
  background-position: right top; 
  background-repeat:no-repeat;    
}
-->     


## Ειδικό Θέμα: Eπεξεργασία δεδομένων βίντεο προσώπου από πολλαπλές πόζες



### Αξελός Χρήστος, ΑΕΜ 1814, 5ο έτος, 10ο Εξάμηνο




***



## Σκοπός Ειδικού Θέματος



* Πειραματική εξέταση του Αλγορίθμου __Random Forests__ στο πρόβλημα του Gaze Recognition


* Αναφορά των <b>δυνατών</b> και <b>αδύνατων</b> σημείων του αλγορίθμου





## Αναφορά στο πρόβλημα του  Gaze Recognition


* <b>Gaze Recognition</b> ή <b>Gaze tracking</b> είναι μια διαδικασία όπου προσπαθούμε να μετρήσουμε την <b>διεύθυνση</b> του βλέμματος ενός ματιού ή το <b>σημείο</b> που αυτό κοιτάζει

<html>
<div style="page-break-before: always;"></div>
</html>

##  Πρακτικές εφαρμογές του Gaze Recognition 



#####  1) Χρήση της τεχνολογίας από άτομα με ειδικές ανάγκες. Το μάτι λειτουργεί ως  χειριστής



* Για κάποιους ανθρώπους τα μάτια ίσως είναι το <b>μοναδικό</b> μέσο επικοινωνίας με το <b>περιβάλλον</b>


* Το μάτι θα μπορούσε να παίξει τον ρόλο απλών μέχρι και πιο πολύπλοκων λειτουργιών, όπως τον ρόλο που έχει το <b>ποντίκι</b> σε έναν υπολογιστή [<b>7</b>] ή ένας <b>διάκοπτης</b>   


#####  2) Χρήση της τεχνολογίας για να ελέγξουμε κατά πόσο κάποιος είναι συγκεντρωμένος σε κάτι



* Για παράδειγμα, μπορούμε να καταλάβουμε κατά <b>πόσο</b> οι θεατές μιας παράστασης βρίσκουν <b>ενδιαφέρουσα</b> την παράσταση [<b>8</b>]


* Μια ιστιοσελίδα που φιλοξενεί <b>διαφημίσεις</b>, θα μπορούσε να μετρά το αντίκτυπο της διαφήμισης με βάση το <b>χρόνο</b> που την κοιτάει το μάτι αντί για τον αριθμό των επισκέψεων της σελίδας   



#####  3) Χρήση της τεχνολογίας για να προβλέψουμε την δράση κάποιου ατόμου, με δεδομένο το Gaze


* Αν μελετήσουμε μια <b>αλληλουχία κινήσεων</b> των ματιών, μπορούμε να <b>προβλέψουμε</b> τις δράσεις ενός ατόμου


* Σύμφωνα με το [<b>9</b>], αυτό μπορεί να εφαρμοστεί σε <b>καθημέρινες</b> εργασίες(διάβασμα, σερφάρισμα στο internet)




#####  4) Διάφορες άλλες εφαρμογές


* Gaze Tracking κατά τη διάρκεια κάθε είδους <b>προσομοιώσεων</b>


* Χρήση της τεχνολογίας του gaze tracking από συστήματα <b>αυτοκινήτων</b> με στόχο την πρόβλεψη <b>δράσεων</b> του οδηγού


* Χρήση της τεχνολογίας του gaze tracking από τη βιομηχανία <b>ηλεκτρονικών παιχνιδιών</b>



<html>
<div style="page-break-before: always;"></div>
</html>

***



## Δεδομένα

* Ως δεδομένα επέλεξα το <a href="https://www.mpi-inf.mpg.de/de/abteilungen/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/"
target="_blank">MPIIGaze Dataset</a>[<b>3</b>]. Ωστόσο υπάρχουν κι'άλλα dataset, όπως το <a href="https://www.idiap.ch/dataset/eyediap" target="_blank">Eyediap</a> και το <a href="http://www.hci.iis.u-tokyo.ac.jp/datasets/" target="_blank">Multiview Dataset</a> [<b>2</b>].


* Οι αρχικές εικόνες έχουνε κανονικοποιηθεί με τέτοιο τρόπο, ώστε να εξετάζονται όλες οι εικόνες υπό τις __ίδιες συνθήκες__ .Επίσης κάθε μάτι εξετάζεται __ανεξάρτητα__ από το άλλο.


* Τα δεδομένα που έχουμε στην διάθεση μας είναι:

    1. Οι εικόνες __e__ του κάθε ματιού με διαστάσεις (W,H) = (60,36)

	2. __Ηead Pose__, 2d διάνυσμα γωνιών σε radians(γωνία Theta και γωνία
Phi)

	3. __Gaze__(2d διάνυσμα επίσης σε radians) το όποιο προσπαθούμε να κάνουμε predict. Κάθε μάτι γίνεται predict __ανεξάρτητα__ από το άλλο

	4. Η γωνία __Theta__ εκφράζει την οριζόντια θέση του κεφαλιού. Για
παράδειγμα αν το κεφάλι έχει προσανατολισμό  προς τα __δεξιά__, θα έχει
__θετική__ τιμή, ενώ αν κοιτάει προς τα __αριστερά__, θα έχει __αρνητική__.

	5. Η γωνία __Phi__ λειτουργεί σαν την Theta, αλλά για τον κάθετο άξονα.
Για παράδειγμα, αν το κεφάλι έχει προσανατολισμό προς τα __πάνω__, θα έχει
__θετική τιμή__, ενώ αν κοιτάει προς τα __κάτω__, θα έχει __αρνητική__

	6. Και οι 2 αυτές γωνίες κυμαίνονται στο διάστημα [-30, +30] σε
__μοίρες__


* Για τον αλγόριθμο Random Forest, κάνουμε __reshape__ τις εικόνες των ματιών
  από (W,H) = (__60,36__) σε (__15,9__) τόσο για το __training__, όσο και για το __testing__  

<html>
<div style="page-break-before: always;"></div>
</html>

***

## Υλοποίηση Αλγορίθμου


* Για την υλοποίηση του  αλγορίθμου, βασίστηκα στην αρχική υλοποίηση του Breiman[<b>1</b>], κάνοντας κάποιες αλλαγές στον τρόπο που διαλέγουμε τα __features__ κατά το split


* Πραγματοποιήσα όμως κάποιες αλλαγές σύμφωνα με τον Sugano [<b>2</b>], ο οποίος χρησιμοποίησε μια <b>παραλλαγή</b> των Random Forests που λέγεται <b>Redundant Random Forests</b>


* Αντίθετα με τον κλασσικό αλγόριθμο των Random Forests του Breiman [<b>1</b>], όπου υπάρχει <b>ένα</b> δάσος και κάθε δέντρο ανήκει σε <b>ένα</b> δάσος, στα <b>Redundant Random forests</b> δημιουργούνται <b>πολλά</b> δάση και ένα δέντρο ανήκει σε <b>παραπάνω από ένα</b> δάση


* Παρακάτω εξηγώ αναλυτικά τον αλγόριθμο αυτόν


<html>
<div style="page-break-before: always;"></div>
</html>

***


## Ομαδοποίηση των δεδομένων με βάση τα Head Poses


* Για την υλοποίηση του  αλγορίθμου, αρχικά ομαδοποιούμε τα training samples σε __P pose clusters__, με βάση το __Head Pose__


* Κάθε Cluster έχει ένα __κέντρο__, το οποίο αποτελείται από ένα διάνυσμα
  (__theta, phi__)


* Για να θεωρηθεί ένα διανύσμα (__theta, phi__) ως κέντρο ενός Cluster, θα πρέπει να <b>μην απέχει</b> απόσταση μικρότερη από <b>Χ</b> από τα ήδη υπάρχοντα κέντρα(πχ στο παρακάτω σχήμα χρησιμοποιώ __Χ=0.08__ και δημιουργούνται __106 Clusters__).


* Όσο __μικρότερο__ το Χ, τόσο πιο __πολλά__ Clusters δημιουργούνται</br>

<html><br>
<div id="foto" style="text-align: center;">
   <img src="centers.jpg"  width="900" alt="foto1">
        <figcaption><b>Εικόνα 1</b>:<i> Διάγραμμα που απεικονίζει τα  <b>Head Poses</b> όλων των σημείων του Training Phase. Με <b>πράσινο</b> χρώμα απεικονίζονται τα κέντρα των Clusters, ενώ με <b>μπλέ</b> χρώμα τα υπόλοιπα σημεία. Η παραπάνω εικόνα χρησιμοποιεί <b>44640</b> training δείγματα, ενώ τα <b>κέντρα</b> απέχουν μεταξύ τους απόσταση <b>μεγαλύτερη</b> των <b>0.03 radians</b>(1.718873) μοίρες)</i></figcaption>
</div>
</html>



<html>
<div style="page-break-before: always;"></div>
</html> 

***

## Κατασκευή του δάσους μέσα από Regression Decision Trees

* Χρησιμοποιώ την __bootstrap__ διαδικασία, επιλέγοντας τυχαία inputs


* Δημιουργούμε τόσα __δέντρα__, όσα και τα __Pose Clusters__, δηλαδή P


* Κάθε δέντρο παίρνει training data από τα __R-nearest Clusters__. Δηλαδή R Clusters
  με τα __κοντινότερα__ Head Poses


* Ως __error__ παίρνουμε το __μέσο gaze error__ από όλα τα regression trees.


<div id="foto" style="text-align: center;">
   <img src="visualization.jpeg" width="500" alt="foto1">
   <figcaption><b>Εικόνα 2</b>:<i> Παράδειγμα όπου <b>γειτονικά Clusters</b> συνεισφέρουν στην κατασκευή ενός δέντρου. Στα Clusters ανοίκουν δείγματα με <b>παρόμοια Head Poses</b></i></br></figcaption>
</div>



<html>
<div style="page-break-before: always;"></div>
</html>

***

## Πώς εκπαιδεύεται το κάθε δέντρο του δάσους


* Σε κάθε κόμβο ενός δέντρου, προσπαθούμε να μάθουμε __συναρτήσεις__ της μορφής

$$
    f = px1 - px2
$$




* Τα px1, px2 είναι οι __Gray__ τιμές από 2 pixel της eye Image (W=15,H=9).


* Τα __pixels__ αυτά μαθαίνονται μέσα από το training. 


* Επίσης προσπαθούμε να "μάθουμε" το __βέλτιστο threshold τ__ για κάθε κόμβο, όπου:

    1. αν $ f < τ $, τότε το training sample κατευθύνεται στο __αριστερό__ υποδέντρο
    
	2. αν $ f >= τ $, τότε κατευθύνεται στο __δεξιό__ υποδέxντρο


* Ο αλγόριθμος με τον οποίο υπολογίζουμε ποια είναι τα __βέλτιστα pixels__ και το __βέλτιστο threshold__ για το split σε __κάθε κόμβο__ του δέντρου είναι το __minimum residual sum of squares__


$$ \begin{align}
 error =\sum_{\substack{i:f_{j}\lt{thres}}}^{nleft}  (g_{i} - \hat{ m_{left} } )^2 + \sum_{\substack{i:f_{j}\ge{thres}}}^{nright} (g_{i} - \hat{ m_{right} } )^2\end{align} $$

* Τα $nleft$ και $nright$ είναι ο __αριθμός__ των δειγμάτων που θα είχε κάθε υποδέντρο, σε περίπτωση που γινόταν το split με βάση τα $px1$,$px2$,$thres$


* Τα  $\hat{ m_{right} }$ και $\hat{ m_{left} }$ είναι η __μέση τιμή__ των gazes που ανήκουν στο __δεξί__ και __αριστερό__ υποδέντρο


* Διαλέγουμε τα $px1$,$px2$,$thres$ που __ελαχιστοποιούν__ το παραπάνω άθροισμα


<html><br>
<div id="foto" style="text-align: center;">
   <img src="stigmiotupo.png" width="800" alt="foto1">
    <figcaption><b>Εικόνα 3:</b><i>Στιγμιότυπο υποδέντρου <b>10 δειγμάτων</b>. Ανάλογα με τις τιμές των <b>Pixel</b> του δείγματος, το τελευταίο θα οδηγηθεί σε έναν <b>τερματικό κόμβο</b>(φύλλο)</i></br></br> </figcaption>
</div>
</html>


<html>
<div style="page-break-before: always;"></div>
</html>

* Γενικότερα στα <b>random forests</b>(Breiman 1984, regression and classification trees), σε προβλήματα regression, προτιμάται ο αλγόριθμος των <b>resindual sum of squares</b>. Αντίθετα με τα <b>regression</b> forests, στα <b>classification</b> προβλήματα η κατασκευή των δέντρων βασίζεται σε άλλες μεθόδους όπως το <b>tree entropy/gain</b> ή το <b>Gini impurity index</b>



* Στόχος των <b>sum of squares</b> είναι να δημιουργηθεί το split, ώστε να πετυχαίνεται στα υποδέντρα το <b>μικρότερο δυνατό variance</b>


* Ο τρόπος μάθησης των στοιχείων διαχωρισμού περιγράφεται παρακάτω:
 
<html><br>
<div id="foto" style="text-align: center;">
   <img src="pseudocode.png" alt="foto1">
    <figcaption><b>Εικόνα 4:</b><i>Στιγμιότυπο υποδέντρου <b>10 δειγμάτων</b>. Ανάλογα με τις τιμές των <b>Pixel</b> του δείγματος, το τελευταίο θα οδηγηθεί σε έναν <b>τερματικό κόμβο</b>(φύλλο)</i></br></br> </figcaption>
</div>
</html>




<!--
      
                1. Για κάθε δυνατό ζευγάρι pixel(px1,px2)
                    2. Για κάθε threshold
                        3. Υπολόγισε το rightError= sum of squares error στο δεξί υποδέντρο  
                        4. Υπολόγισε το leftError για το αριστερό υποδέντρο
                        5. Error = rightError + leftError
                        6. Αν Error < minError
                            7. minError = Error
                            8. minPx1 = px1
                            9. minPx2 = px2
                            10. minThreshold = threshold
-->                                  




* Οπότε έτσι μαθαίνουμε τα  minPx1, minPx2, minThreshold κάθε κόμβου



<html>
<div style="page-break-before: always;"></div>
</html>

***



## Πώς γίνεται το testing



* Μόλις θέλουμε να ελέγξουμε ένα testing sample, δεν το στέλνουμε σε όλα τα
  δέντρα, αλλά στα __R-nearest δέντρα__ με βάσει το head pose


* Τότε υπολογίζουμε το __average error__ από τα R-nearest regression δέντρα.


* Μας ενδιαφέρει ωστόσο να γνωρίζουμε και την __τυπική απόκλιση__(standard deviation), για να βλέπουμε __πόσο κοντά__ είναι οι προβλέψεις μας στο __mean error__


***

## Πειραματική Αξιολόγηση Αλγορίθμου



* Κατά την αναλυτική αξιολόγηση του Αλγορίθμου θα πρέπει να απαντήσουμε τα εξής ερωτήματα:

    1. Ποιός είναι ο βέλτιστος __αριθμός__ από __Clusters__ ή ισοδύναμα ποιά θα είναι η __μικρότερη δυνατή απόσταση__ μεταξύ 2 κέντρων

    2. Ποιός είναι ο βέλτιστος __αριθμός γειτόνων__ κάθε Cluster

    3.  Ποιός είναι ο βέλτιστος __αριθμός δειγμάτων__ εκπαίδευσης

    4. Κατά τη διαδικασία δημιουργίας υποδέντρων σε ένα δέντρο, πόσες __μεταβλητές διαχωρισμού__ χρησιμοποιούμε?

    5. Πόσο __βάθος__ πρέπει να έχει το κάθε δέντρο

    6. Αν σε ένα δέντρο καταλήξουμε σε φύλλο όπου υπάρχουν  __παραπάνω από ένα__ δείγματα, πώς προσδιορίζεται η 2d-gaze του συγκεκριμένου δέντρου


##### Ερώτημα 1ο: Εύρεση μικρότερης απόστασης Κέντρων/Αριθμός Clusters


* Επειδή δεν έχουμε υπολογίσει ακόμα τον __βέλτιστο αριθμό γειτόνων__, θεωρώ αρχικά πως __R = 5__ γείτονες.


* Επίσης, για να <b>μειώσω</b> τον χρόνο εκπαίδευσης, χρησιμοποιώ αρχικά __10,000 training samples__


* Ο αριθμός των __μεταβλητών διαχωρισμού__ που θα χρησιμοποιούσαμε είναι <b>WIDTH</b> * <b>HEIGHT</b> * <b>THRESHOLD_RANGE</b> = 15 * 16 * 255. Επειδή ο αριθμός αυτός είναι υπερβολικά <b>μεγάλος</b>, σύμφωνα με το paper του <b>Breiman</b> αρκεί χρησιμοποιήσουμε την __τετραγωνική ρίζα__ αυτού του αριθμού


* Τέλος, αν ένα φύλλο ενός δέντρου περιέχει __παραπάνω από ένα__ training samples, υπολογίζω απλώς τον __μέσο όρο__ των __2-d gazes__ και προκύπτει ένα 2d-gaze διάνυσμα


* <b>Δεν</b> περιορίζουμε το βάθος του κάθε δέντρου


* Με βάση τα παραπάνω, υπολογίζουμε το __mean error__ και το __standard deviation__

<html><br>
<div id="foto" style="text-align: center;">
    <img src="graph1.jpg" height="400" width="600" alt="foto1">
    <figcaption><b>Εικόνα 5:</b><i>Διάγραμμα με οριζόντιο άξονα την <b>ελάχιστη απόσταση</b> σε rad ανάμεσα σε 2 κέντρα των γειτονικών Cluster συναρτήσει του <b>μέσου σφάλματος</b> και της <b>τυπικής απόκλισης</b>. Για απόσταση <b>d=0.05</b> rad παρατηρούμε το ελάχιστο μέσο σφάλμα. H τυπική απόκλιση είναι <b>υψηλή</b>(=5.31 μοίρες)</i></br></br> </figcaption>
</div>
</html>
  

  


* Από το γράφημα παρατηρούμε πως το <b>mean error</b> γίνεται ελάχιστο για όταν η minimum απόσταση ανάμεσα στα κέντρα είναι <b>0.05 degrees</b>(οριζόντια + κάθετα)


* Ωστόσο η <b>τυπική απόκλιση</b> είναι αρκετά μεγάλη(5.38 μοίρες), πράγμα που σημαίνει πως οι προβλέψεις έχουν αρκετή <b>διαφορά</b> μεταξύ τους ως προς τον μέσο όρο


* Κρατάμε ως απόσταση το 0.05 μεταξύ των κεντρών. Θέτοντας την απόσταση των κεντρών(των Cluster) 0.05, για 10,000 δείγματα εκπαίδευσης προκύπτουν <b>238 κέντρα</b> 




##### Ερώτημα 2ο: Βέλτιστος Αριθμός γειτονικών Cluster/ αριθμός δέντρων



* Ο συνολικός αριθμός των <b>Cluster</b> είναι ίσος με τον συνολικό αριθμό <b>δέντρων</b>


* Κρατάμε ως δεδομένα την <b>minimum απόσταση</b> μεταξύ 2 κέντρων(=<b>0.05</b> rads) που βρήκαμε στο προηγούμενο ερώτημα και συνεχίζουμε τις μετρήσεις, αναζητώντας τον ιδανικό <b>αριθμό γειτονικών Cluster</b> που θα συνεισφέρουν στην εκπαίδευση ενός δέντρου.



* Όπως και στο προηγούμενο ερώτημα, ο αριθμός των δειγμάτων εκπαίδευσης παραμένει <b>10,000</b>


<html><br>
<div id="foto" style="text-align: center;">
    <img src="graph2.jpg" height="400" width="600" alt="foto1">
    <figcaption><b>Εικόνα 6:</b><i>Διάγραμμα με οριζόντιο άξονα τον <b>αριθμό γειτόνων</b> που συνεισφέρουν στην εκπαίδευση ενός δέντρου συναρτήσει του <b>μέσου σφάλματος</b> και της <b>τυπικής απόκλισης</b>. Το ελάχιστο μέσο σφάλμα παρατηρείται για <b>R = 30</b>. Από το σημείο αυτό και μετά η μείωση θεωρείται αμελητέα. H τυπική απόκλιση εξακολουθεί να παραμένει αρκετά <b>υψηλή</b>(περίπου 5 μοίρες)</i></br></br></figcaption>
</div><br>
</html>


* Παρατηρούμε πως η <b>αύξηση</b> του αριθμού των <b>γειτόνων</b>(μέχρι ένα σημείο) προκαλεί <b>μείωση</b> του <b>σφάλματος</b>


* Αυτό συμβαίνει διότι καθώς <b>αυξάνουμε</b> τον αριθμό γειτόνων, <b>περισσότερα</b> δέντρα συνεισφέρουν στο τελικό αποτέλεσμα που θέλουμε να προβλέψουμε


* Έτσι, παίρνοντας ως αποτέλεσμα την <b>μέση</b> πρόβλεψη όλων των δέντρων, <b>μικραίνει</b> η πιθανότητα να έχουμε <b>λανθασμένες</b> προβλέψεις


* Αυτό είναι το σημείο που <b>υπερτερούν</b> τα random forests σε σχέση με τα decision trees. Τα <b>decision trees</b> από την φύση τους μπορούν <b>εύκολα</b> να κάνουν μία <b>λάθος</b> πρόβλεψη. Στα <b>random forests</b>, αν ένα δέντρο κάνει μια "κακή πρόβλεψη", τα υπόλοιπα δέντρα είναι σε θέση να <b>διορθώσουν</b> την πρόβλεψη αυτή, αφού παίρνουμε τον <b>μέσο όρο</b> όλων των δέντρων


* Γενικά, όσο πιο <b>πολλά</b> είναι τα δέντρα τόσο πιο <b>καλή</b> θα είναι η πρόβλεψη. Απλά από ένα σημείο και μετά <b>δεν</b> θα έχει νόημα να <b>αυξάνουμε</b> τα δέντρα, γιατί η αύξυση θα γίνεται <b>ελάχιστη</b> σε σχέση με τον επιπρόσθετο <b>χρόνο</b> εκπαίδευσης που χρειαζόμαστε(<b>συγκλίνει</b> δηλαδή σε κάποιο αριθμό η επίδοση)



##### Ερώτημα 3ο: Ποιός είναι ο βέλτιστος αριθμός δειγμάτων εκπαίδευσης


* Θεωρώντας δεδομένο τον <b>αριθμό γειτόνων</b>(R=5) και την <b>ελάχιστη απόσταση</b> των κέντρων των Clusters(dist=0.05), προσπαθούμε να βρούμε τον βέλτιστο <b>αριθμό δειγμάτων</b> εκπαίδευσης.




<html><br>
<div id="foto" style="text-align: center;">
    <img src="graph3.jpg" height="400" width="600" alt="foto1">
    <figcaption><b>Εικόνα 6:</b><i>Διάγραμμα με οριζόντιο άξονα τον <b>αριθμό δειγμάτων εκπαίδευσης</b> συναρτήσει του <b>μέσου σφάλματος</b> και της <b>τυπικής απόκλισης</b>. Το μέσο σφάλμα <b>συγκλίνει</b> στην τιμή <b>6.49</b> μοίρες, ενώ η τυπική απόκλιση παραμένει γύρω στις <b>4.7</b> μοίρες</i></br></br> </figcaption>
</div>
</html>





 * Σύμφωνα με τον <b>Breiman</b>, τα random forests <b>αποφεύγουν</b> το <b>overfitting</b> καθώς αυξάνεται το <b>training sample</b>



* Οπότε παίρνουμε ως ολικό <b>μέσο</b> σφάλμα <b>6.49247</b> μοίρες με <b>τυπική απόκλιση</b> ως προς αυτό <b>4.70</b> μοίρες




##### Ερώτημα 4ο: Εύρεση αριθμού μεταβλητών διαχωρισμού(split features)



* <b>Μεταβλητές διαχωρισμού</b>(split features) είναι μεταβλητές οι οποίες καθορίζουν <b>πώς</b> θα γίνει το split(διαχωρισμός training samples) σε κάθε κόμβο του δέντρου. 


* Στο συγκεκριμένο πρόβλημα, χρησιμοποιούμε τις εξής <b>μεταβλητές διαχωρισμού</b>:

    1. Τις <b>συντεταγμένες</b>(height,width) του <b>pixel 1</b>
    
	2. Τις <b>συντεταγμένες</b>(height,width) του <b>pixel 2</b>
    
    3. Την <b>απόσταση</b>(threshold) ανάμεσα στις <b>gray</b> τιμές(0 -> 255 εύρος) αυτών των 2 pixel


* Υπάρχουν <b>height*width</b> πιθανές τιμές που μπορούν να πάρουν  η 1η και η 2η μεταβλητή 


* Η μεταβλητή <b>threshold</b> έχει εύρος τιμών(thres_range) από <b>0</b> μέχρι <b>50</b>(αυθαίρετο το 50)


* Κατά την διαδικασία της εκμάθησης, είπαμε πως προσπαθούμε να μάθουμε τί <b>τιμή</b> θα έχουν αυτές οι μεταβλητές σε κάθε <b>μή-τερματικό</b> κόμβο του κάθε δέντρου 


* Ωστόσο, υπολογιστικά θα ήταν δύσκολο να δοκιμάσω  $thresholdRange * (height*width)^2 = 50*(9*15)^2 = 911,250$ <b>διαφορετικές</b> παραμέτρους, ώστε να τις χρησιμοποιήσω στον τύπο για τον υπολογισμό του minimum square error


* Αυτό λύνεται σύμφωνα με τον <b>Breiman</b>(2001), o oποίος αναφέρει πως δεν χρειάζεται να δοκιμάσουμε <b>όλους</b> τους συνδιασμούς των παραμέτρων. Αρκεί να χρησιμοποιήσουμε την <b>τετραγωνική ρίζα</b> αυτών, επιλέγοντας <b>τυχαίους</b> συνδιασμούς(στην <b>εικόνα 4</b>, o αλγόριθμος ανταποκρίνεται για τους <b>911,250</b> συνδιασμούς. Στην πράξη όμως χρησιμοποιούμε την <b>τετραγωνική ρίζα</b> αυτών) 


* Πράγματι, παρόλο που <b>δεν</b> πραγματοποιούμε όλους τους συνδιασμούς, η <b>απόδοση</b> του αλγορίθμου παραμένει η <b>υψηλότερη</b> όταν χρησιμοποιούμε την τετραγωνική ρίζα των συνδιασμών




##### Ερώτημα 5ο: Βέλτιστο βάθος κάθε δέντρου


* Ο πιο διαδεδομένος τρόπος για περιορισμό του <b>βάθους</b> ενός δέντρου είναι να θεωρήσουμε πως σε ένα φύλλο πρέπει να υπάρχουν <b>τουλάχιστον N</b> δείγματα, με N >= 1. 


* Έτσι μπορούμε να περιορίσουμε το <b>μέγεθος</b> ενός δέντρου, καθώς και να μειώσουμε τον <b>χρόνο εκπαίδευσης</b> του


* Πραγματοποίησα μετρήσεις όπου πείραζα τον αριθμό <b>Ν</b>. Παρατήρησα όμως ότι για <b>Ν <= 7</b> η απόδοση ήταν <b>ίδια</b>, ενώ για <b>Ν > 7</b> η απόδοση <b>χειροτέρευε</b>


* Σύμφωνα με τον <b>Breiman</b>(2001), στα <b>random forests</b> το βάθος κάθε δέντρου <b>δεν</b> θα πρέπει να περιορίζεται


* Αντίθετα με τα <b>δέντρα αποφάσεως</b> όπου χωρίς περιορισμό του βάθους υπάρχει κίνδυνος <b>overfitting</b> [<b>5</b>], στα random forests <b>δεν</b> είναι τόσο σημαντίκος


* Ο λόγος είναι το ότι αν περιορίσουμε το <b>μέγεθος</b> των δέντρων, τότε τα δέντρα θα αρχίσουν να <b>μοιάζουν</b> μεταξύ τους, οπότε δεν έχει νόημα το έχουμε πολλά <b>όμοια</b> δέντρα


* Πολλά <b>όμοια</b> δέντρα σημαίνει ότι όλα τα δέντρα παίρνουν την <b>ίδια</b> απόφαση, οπότε ο αλγόριθμος μετατρέπεται στον αλγόριθμο των <b>decision trees</b> αλλά με πολλά <b>ίδια</b> δέντρα






##### Ερώτημα 6ο: Πώς προσδιορίζουμε την 2-gaze ενός δέντρου, αν σε ένα φύλλο υπάρχουν παραπάνω από ένα training samples



* Στους παραπάνω υπολογισμούς, χρησιμοποίησα τον μέσο όρο των 2d-gazes από όλα τα training samples που βρισκόντουσαν σαυτό το φύλλο του δέντρου


* Υπάρχουν όμως κιάλλοι μέθοδοι, όπως αρκετά διαδεδόμενο στην περίπτωση αυτή <b>logistic regression</b> τις οποίες τις αναφέρω ως εναλλακτικές/βελτιστοποιήσεις 


***

## Σύγκριση σε σχέση με τις μετρήσεις των Sugano, Zhang



* Συγκεντρώνωντας όλες τις μετρήσεις που κάναμε, βρήκαμε πως το ελάχιστο <b>μέσο</b> error είναι  <b>6.49247</b> μοίρες με τυπική απόκλιση <b>4.79</b> μοίρες και πετυχαίνεται για αριθμό γειτόνων <b>R = 30</b> αριθμό δειγμάτων <b>80,000</b>



* O Sugano στο [<b>2</b>], χρησιμοποιώντας διαφορετικό database απο εμας(Multiview Dataset), υπολογίζει <b>παρόμοιο</b> με μας ελάχιστο σφάλμα(=<b>6.5</b>) μοίρες. 


* Ωστόσο η τυπική απόκλιση κυμαίνεται <b>πολύ χαμηλότερα</b> από τους υπολογισμούς που πραγματοποιήσαμε(=<b>1.5</b> μοίρες), πράγμα που κάνει τις προβλέψεις του <b>πιο σταθερές</b> ως προς την ορθότητα. 


* Τους βέλτιστους υπολογισμούς τους πετυχαίνει χρησιμοποιώντας <b>R = 5</b> δέντρα, ενώ ο <b>αριθμός των δειγμάτων</b> εκπαίδευσης που χρησιμοποιεί έχει την <b>ίδια επίδοση</b> με τις μετρήσεις που πραγματοποιήσαμε προηγουμένως 



* Από την άλλη ο Zhang στο [<b>3</b>] χρησιμοποιεί το ίδιο database με μας(MPIIGaze). Πετυχαίνει και αυτός παρόμοιο με μας mean error(=<b>6.5</b> μοίρες) ενώ ως τυπική απόκλιση πετυχαίνει κιαυτός γύρω στην <b>1.5</b> μοίρα. Πολύ <b>χαμηλότερη</b> από το δικό μας


* Στα paper [<b>2</b>,<b>3</b>] περιέχονται αναλυτικά <b>διαγράμματα</b> που δείχνουν τα σφάλματα των Sugano και Zhang(Sugano [<b>2</b>], εικόνα 2, cross-subject και Zhang [<b>3</b>], εικόνα 8)  


***

## Πιθανές βελτιώσεις του αλγορίθμου




* Υπάρχουν κάποιες πιθανές βελτιστοποιήσεις τις οποίες <b>δεν</b> δοκίμασα, αλλά θα μπορούσαν να επιφέρουν <b>βελτίωση</b> στην απόδοση 




#####   1) Εισαγωγή μεθόδων για hyperparameter tuning(cross-validation, grid-random search)


* Σύμφωνα με τον <b>Breiman</b>(2001), <b>δεν</b> είναι απαραίτητο το cross-validation στα Random Forests, λόγω του <b>Bagging</b> που <b>μειώνει</b> την διασπορά


* Εφαρμόζοντας όμως <b>K-fold</b> Evaluation μπορούμε να <b>μειώσουμε</b> την διασπορά(σύμφωνα με το [<b>6</b>]), πράγμα που αποτελεί το πρόβλημα στους υπολογισμούς μας.  


* Από την άλλη πλευρά, στο συγκεκριμένο πρόβλημα  ο αριθμός των features είναι πολύ <b>μεγάλος</b> σε σχέση με τα training data για να εφαρμόσουμε το <b>grid search</b>, λόγω του ότι και απαιτεί πολύς <b>χρόνος</b>. 


* Λόγω της <b>τυχαιότητας</b> και του ότι κάθε φορά χρησιμοποιούμε την <b>τετραγωνική ρίζα</b> όλων των πιθανών αναζητήσεων(=<b>911,250</b> χιλιάδες) για την κατασκευή ενός <b>κόμβου</b>, μπορούμε να πούμε πως εφαρμόζουμε το <b>random search</b>


* Τέλος, σύμφωνα με το [<b>6</b>], η μέθοδος <b>Sequential model-based optimization</b>(SMBO) αποτελεί μία μέθοδο λύσης προβλημάτων <b>βελτιστοποίησης</b>( πακέτο  <a href="https://cran.r-project.org/web/packages/tuneRanger/index.html"
target="_blank">TuneRanger</a> στην γλώσσα <b>R</b>), 



#####   2) Μείωση του αριθμού των features 


* Όπως αναφέραμε και στο <b>ερώτημα 4</b>, ο αριθμός των features που χρησιμοποιούμε είναι αρκετά <b>μεγάλος</b>(=911,250). 


* Αν και ο Breiman(2001) θεωρεί πως η εισαγωγή της <b>τυχαιότητας</b> κατά την επιλογή των features σε συνδιασμό με τον <b>μεγάλο αριθμό δέντρων</b>(γειτόνων στο δικό μας πρόβλημα) μπορεί να <b>εξαλείψει</b> το πρόβλημα του μεγάλου αριθμού από features, θα μπορούσαμε ωστόσο πειραματικά να εφαρμόσουμε αλγορίθμους που <b>μειώνουν</b> τον αριθμό των <b>features</b>(dimensionality reduction) πριν τρέξουμε τον αλγόριθμο μας(πχ. PCA, LDA) και να ελέγξουμε κατά πόσο οι αλγόριθμοι αυτοί μειώνουν το <b>ολικό error</b>. 


* Με αυτόν τον τρόπο, θα μπορούσαμε κατά την κατασκευή ενός <b>μη-τερματικού</b> κόμβου να κάνουμε <b>απαλοιφή</b> κάποιων features ώστε να επικεντρωθούμε σε <b>σημαντικά</b> features


* Υπάρχει ωστόσο ο κίνδυνος να δημιουργούνται κατά το training <b>όμοια</b> δέντρα, πράγμα που αφαιρεί το νόημα του αλγορίθμου(καταλήγουμε στα decision trees, όπου η διακύμανση είναι μεγάλη)



#####    3) Κάθε πρόβλεψη ενός δέντρου έχει έναν συντελεστή "αυτοπεποίθησης"


* Η τεχνική αυτή παρουσιάζεται στο [<b>4</b>], όπου κάθε <b>πρόβλεψη</b> ενός μεμονωμένου δέντρου, συνοδεύεται από έναν συντελεστή όμοιο με την <b>τυπική απόκλιση</b>


* Αν στο <b>τερματικό φύλλο</b> ενός δέντρου ανήκουν <b>παραπάνω από 1</b> training samples, υπολογίζουμε την <b>τυπική απόκλιση</b> των training samples αυτών ως προς την <b>μέση τιμή</b>


* Αν η τιμή της τυπικής απόκλισης <b>ξεπερνά</b> μια <b>προκαθορισμένη</b> τιμή, τότε η πρόβλεψη θεωρείται <b>αναξιόπιστη</b> και το δάσος <b>απορρίπτει</b> την πρόβλεψη αυτού του δέντρου. 


* Ο <b>τελικός</b> μέσος όρος στην περίπτωση αυτή υπολογίζεται <b>χωρίς</b> να λαμβάνουμε υπ'όψη το δέντρο αυτό


* Με αυτόν τον τρόπο, <b>απορρίπτουμε</b> προβλέψεις οι οποίες είναι <b>αμφίβολες</b>, καθώς δεν αντικατοπτρίζεται ο μέσος όρος από τα training samples


#####   4) Χρήση κάποιας συνάρτησης αντί του μέσου όρου στα φύλλα των δέντρων


* Στις περιπτώσεις όπου το φύλλο ενός δέντρου περιέχει <b>παραπάνω από ένα</b> training samples, θα μπορούσαμε να χρησιμοποιήσουμε για τον προσδιορισμό του τελικού 2d-gaze διανύσματος κάποια <b>συνάρτηση</b>, αντί για τον υπολογισμού του <b>μέσου</b> gaze


* Ανάλογα με την <b>κατανομή</b> των δεδομένων, θα μπορούσαμε να επιλέξουμε την κατάλληλη <b>συνάρτηση</b>, όπως για παράδειγμα logistic regression, SVMs, ή non-linear συναρτήσεις


***


## Λογισμικό που σχετίζεται με τα random forests



* Για να πραγματοποιηθούν οι παραπάνω μετρήσεις, υλοποίησα τον αλγόριθμο των Random Forests αρχικά σε <b>Matlab</b>. Ωστόσο επειδή η απόδοση <b>δεν</b> ήταν η αναμενόμενη, προχώρησα στην υλοποίηση του αλγορίθμου σε <b>C/C++</b> ώστε να στοχεύσω στην <b>χρονική απόδοση</b> 


* Ο λόγος που προχώρησα στην <b>υλοποίηση</b> του αλγορίθμου αντί να χρησιμοποιήσω κάποια από τα παρακάτω <b>έτοιμα</b> πακέτα είναι πως μέσα από την υλοποίηση θα υπήρχε καλύτερη <b>κατανόηση</b> του αλγορίθμου. Επιπλέον επειδή είναι ένας σχετικά <b>εύκολα υλοποιήσιμος</b> αλγόριθμος σε σχέση με άλλους αλγορίθμους, η υλοποίηση έγινε σε <b>λογικά χρονικά πλαίσια</b>


* Το λογισμικό που υλοποιεί τον αρχικό σχεδιασμό των random forests όπως ο <b>Breiman</b>(2001) είχε ορίσει, βρίσκεται στην ιστιοσελίδα του <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm"
target="_blank">Berkeley</a> και είναι γραμμένο σε <b>Fortran</b>


* Αρκετά δημοφιλές είναι το πακέτο <a href="https://cran.r-project.org/web/packages/randomForest/index.html"
target="_blank">Random Forests</a> σε γλώσσα <b>R</b> 


* Υπάρχει επίσης για <b>Python</b>, από το πακέτο <b>Scikit-learn</b> συναρτήσεις για τα <a href="http://scikit-learn.org/stable/modules/ensemble.html#random-forests" target="_blank">Random Forests</a> 


*  Σε <b>Matlab</b>, υπάρχει η συνάρτηση <a href="http://in.mathworks.com/help/stats/treebagger.html?refresh=true"
target="_blank">TreeBagger</a>


* Τέλος, υπάρχει η κλάση <a href="https://docs.opencv.org/2.4/modules/ml/doc/random_trees.html"
target="_blank">RandomTrees</a> της <b>OpenCV</b> για υλοποίηση σε <b>C++</b>


***


## Συμπεράσματα


* Τα Random Forests, φαίνεται να έχουν μια <b>σχετικά καλή</b> απόδοση



* Μεγάλο τους πλεονέκτημα είναι το γεγονός ότι μπορούν να εκπαιδευτούν <b>πολύ γρήγορα</b>. Η εκπαίδευση του κάθε δέντρου είναι <b>ανεξάρτητη</b> από τα άλλα, πράγμα που δίνει μεγάλα περιθώρια για <b>παραλληλοποίηση</b>


* Η υλοποίηση τους είναι σχετικά <b>εύκολη</b> σε σχέση με άλλους αλγορίθμους. 


* Σύμφωνα με τους υπολογισμούς του <b>Zhang</b>[<b>3</b>], η χρήση ενός CNN(Convolutional Neural Network) θα μπορούσε να πετύχει μεγαλύτερη ορθότητα(<b>6.2</b> degree error σε σχέση με το <b>6.5</b> των Random Forests με παρόμοια τυπική απόκλιση(=<b>1.5 μοίρες</b>) 


* Ο μεγάλος αριθμός των <b>features</b> αποτελεί την <b>κύρια</b> αιτία <b>μείωσης</b> της απόδοσης του αλγορίθμου


* Φαίνεται πως η γλώσσα <b>R</b> παρέχει τις περισσότερες <b>δυνατότητες</b> για την χρήση αυτού του αλγορίθμου. Εξίσου καλά μπορεί όμως να αποδώσει και η <b>Python</b> 


***

## Μελλοντική εργασία


* Ενδιαφέρον παρουσιάζει η διαδικασία με την οποία έγινε η <b>συλλογή</b> των <b>δεδομένων</b>, καθώς και το Normalization(κανονικοποίηση) των δεδομένων. Ο Sugano [<b>2</b>] παρουσιάζει <b>λεπτομερώς</b> την διαδικασία όπου κανονικοποιεί τα δεδομένα(Normalization) καθώς και την <b>αντιστοίχιση</b> των Pixel από τον <b>3d</b> στον <b>2d</b> χώρο( Camera Calibration )


* Θα μπορούσε επίσης να γίνει η παρουσίαση μια λεπτομερούς <b>εφαρμογής</b>, όπως αυτές που ανέφερα, όπου η χρήση της τεχνολογίας του Gaze Recognition θα ήταν <b>χρήσιμη</b>


* Επιπλέον, λόγω του ότι ο Zhang [<b>3</b>] παρουσιάσε μια λύση με <b>μικρότερο</b> μέσο σφάλμα χρησιμοποιώντας το <b>ίδιο</b> database με μας, θα μπορούσαμε να εξετάσουμε το πρόβλημα με την χρήση των <b>CNNs</b>(Convolutional Neural Networks)


***


## Aναφορές σε βιβλιογραφίες/δημοσιεύσεις

1. Breiman, L., Friedman, J.,Olshen, R., and Stone, C. [1984] Classification and Regression Trees,  Wadsworth
2. Y. Sugano, Y. Matsushita, and Y. Sato. [2014] Learning-by-synthesis for appearance-based 3d gaze estimation.
3. Z. Zhang, Y.Sugano, M.Fritz, A. Bulling [2015] Appearance-Based Gaze Estimation in the Wild
4. M. Dantone, J. Gall, G. Fanelli, L. Van Gool [2013]:  Real-time  facial feature detection using conditional regression forests.
5. Mansour, Y. [1997], Pessimistic decision tree pruning based on tree size
6. Philipp Probst, Marvin Wright and Anne-Laure Boulesteix. [2018], Hyperparameters and Tuning Strategies for Random Forest
7. P. Majaranta, A. Bulling [2014] Eye Tracking and Eye-Based Human–Computer Interaction
8. Z. Zhang, Y.Sugano, A. Bulling [2016] AggreGaze: Collective Estimation of Audience Attention on Public Displays 
9.  H. Sattar, S. Müller, M. Fritz, A. Bulling [2015] Prediction of Search Targets From Fixations in Open-World Settings



<!--
</style>  
-->    
